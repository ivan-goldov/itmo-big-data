{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "125ce7da-e307-4a15-bd5b-badd7c7ee2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task2.py\n"
     ]
    }
   ],
   "source": [
    "%%file task2.py\n",
    "\n",
    "from mrjob.job import MRJob, MRStep\n",
    "from collections import defaultdict\n",
    "\n",
    "class MRLongestPhrase(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        if not line.startswith('\"character\" \"dialogue\"'):\n",
    "            _, character, phrase = line.split(\" \", 2)\n",
    "            c = character.strip('\"').strip(\"\\\\\")\n",
    "            p = phrase.strip('\"').strip(\"\\\\\")\n",
    "            yield c, len(p)\n",
    "\n",
    "    def reducer_aggregate(self, character, lengths):\n",
    "        yield None, (character, max(lengths))\n",
    "\n",
    "    def reducer(self, _, pairs):\n",
    "        char2len = [(p[0], p[1]) for p in pairs]\n",
    "        yield from sorted(char2len, key=lambda x: -x[1])\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "                reducer=self.reducer_aggregate,\n",
    "            ),\n",
    "            MRStep(reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRLongestPhrase.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ed0946e-b1a4-42f7-acb6-6781e7b365fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task2.root.20231206.202820.981575\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task2.root.20231206.202820.981575/output\n",
      "Streaming final output from /tmp/task2.root.20231206.202820.981575/output...\n",
      "Removing temp directory /tmp/task2.root.20231206.202820.981575...\n",
      "\"LEIA\"\t596\n",
      "\"BIGGS\"\t356\n",
      "\"DODONNA\"\t353\n",
      "\"JABBA\"\t339\n",
      "\"LUKE\"\t318\n",
      "\"TARKIN\"\t302\n",
      "\"THREEPIO\"\t288\n",
      "\"BEN\"\t262\n",
      "\"VADER\"\t257\n",
      "\"HAN\"\t256\n",
      "\"MOTTI\"\t228\n",
      "\"OFFICER\"\t219\n",
      "\"GREEDO\"\t203\n",
      "\"OWEN\"\t191\n",
      "\"SECOND\"\t187\n",
      "\"TAGGE\"\t183\n",
      "\"FIXER\"\t177\n",
      "\"RED\"\t176\n",
      "\"VOICE\"\t148\n",
      "\"DEATH\"\t143\n",
      "\"COMMANDER\"\t112\n",
      "\"MASSASSI\"\t107\n",
      "\"HUMAN\"\t107\n",
      "\"REBEL\"\t106\n",
      "\"TROOPER\"\t101\n",
      "\"IMPERIAL\"\t99\n",
      "\"AUNT\"\t98\n",
      "\"WEDGE\"\t98\n",
      "\"CONTROL\"\t97\n",
      "\"ASTRO-OFFICER\"\t96\n",
      "\"GOLD\"\t93\n",
      "\"BASE\"\t91\n",
      "\"WILLARD\"\t90\n",
      "\"GANTRY\"\t88\n",
      "\"INTERCOM\"\t87\n",
      "\"CAPTAIN\"\t77\n",
      "\"MAN\"\t76\n",
      "\"BERU\"\t72\n",
      "\"FIRST\"\t72\n",
      "\"BARTENDER\"\t68\n",
      "\"CHIEF\"\t65\n",
      "\"CAMIE\"\t38\n",
      "\"TECHNICIAN\"\t36\n",
      "\"WOMAN\"\t32\n",
      "\"CREATURE\"\t28\n",
      "\"DEAK\"\t22\n",
      "\"PORKINS\"\t20\n",
      "\"WINGMAN\"\t9\n"
     ]
    }
   ],
   "source": [
    "!python3 task2.py ../sw-data/SW_EpisodeIV.txt > res4_local.txt && cat res4_local.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b043bb5b-9c90-4444-a5d0-9d46270f24bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task2.root.20231206.202853.803036\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task2.root.20231206.202853.803036/output\n",
      "Streaming final output from /tmp/task2.root.20231206.202853.803036/output...\n",
      "Removing temp directory /tmp/task2.root.20231206.202853.803036...\n",
      "\"YODA\"\t385\n",
      "\"VADER\"\t269\n",
      "\"LEIA\"\t264\n",
      "\"THREEPIO\"\t238\n",
      "\"LANDO\"\t206\n",
      "\"VEERS\"\t205\n",
      "\"LUKE\"\t198\n",
      "\"PIETT\"\t176\n",
      "\"REBEL\"\t167\n",
      "\"NEEDA\"\t160\n",
      "\"HAN\"\t152\n",
      "\"RIEEKAN\"\t121\n",
      "\"ZEV\"\t118\n",
      "\"BEN\"\t117\n",
      "\"DECK\"\t115\n",
      "\"CREATURE\"\t109\n",
      "\"OZZEL\"\t88\n",
      "\"DERLIN\"\t88\n",
      "\"CONTROLLER\"\t85\n",
      "\"DACK\"\t82\n",
      "\"LIEUTENANT\"\t78\n",
      "\"SECOND\"\t77\n",
      "\"EMPEROR\"\t75\n",
      "\"MEDICAL\"\t72\n",
      "\"ASSISTANT\"\t71\n",
      "\"SENIOR\"\t69\n",
      "\"TRACKING\"\t67\n",
      "\"COMMUNICATIONS\"\t64\n",
      "\"INTERCOM\"\t64\n",
      "\"BOBA\"\t58\n",
      "\"IMPERIAL\"\t53\n",
      "\"ANNOUNCER\"\t50\n",
      "\"HEAD\"\t47\n",
      "\"WEDGE\"\t43\n",
      "\"WOMAN\"\t43\n",
      "\"TRENCH\"\t43\n",
      "\"HOBBIE\"\t38\n",
      "\"CAPTAIN\"\t33\n",
      "\"PILOT\"\t29\n",
      "\"JANSON\"\t25\n",
      "\"STRANGE\"\t23\n",
      "\"FIRST\"\t22\n",
      "\"OFFICER\"\t14\n",
      "\"MAN\"\t12\n",
      "\"PILOTS\"\t12\n"
     ]
    }
   ],
   "source": [
    "!python3 task2.py ../sw-data/SW_EpisodeV.txt > res5_local.txt && cat res5_local.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7b2b204-f675-4454-94b6-7e50857c5220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task2.root.20231206.202854.840283\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task2.root.20231206.202854.840283/output\n",
      "Streaming final output from /tmp/task2.root.20231206.202854.840283/output...\n",
      "Removing temp directory /tmp/task2.root.20231206.202854.840283...\n",
      "\"BEN\"\t773\n",
      "\"ACKBAR\"\t503\n",
      "\"LUKE\"\t502\n",
      "\"MON\"\t460\n",
      "\"YODA\"\t293\n",
      "\"EMPEROR\"\t227\n",
      "\"VADER\"\t207\n",
      "\"GENERAL\"\t183\n",
      "\"THREEPIO\"\t183\n",
      "\"COMMANDER\"\t165\n",
      "\"DEATH\"\t158\n",
      "\"HAN\"\t154\n",
      "\"LANDO\"\t149\n",
      "\"NINEDENINE\"\t145\n",
      "\"HAN/PILOT\"\t130\n",
      "\"PIETT\"\t126\n",
      "\"SHUTTLE\"\t124\n",
      "\"LEIA\"\t110\n",
      "\"CONTROLLER\"\t99\n",
      "\"JABBA\"\t94\n",
      "\"ANAKIN\"\t83\n",
      "\"WEDGE\"\t79\n",
      "\"JERJERROD\"\t76\n",
      "\"CONTROL\"\t63\n",
      "\"GUARD\"\t62\n",
      "\"OFFICER\"\t59\n",
      "\"BOUSHH\"\t54\n",
      "\"PILOT\"\t53\n",
      "\"RED\"\t48\n",
      "\"SCOUT\"\t48\n",
      "\"STORMTROOPER\"\t42\n",
      "\"BIB\"\t40\n",
      "\"BUNKER\"\t38\n",
      "\"OOLA\"\t37\n",
      "\"GREEN\"\t34\n",
      "\"REBEL\"\t33\n",
      "\"GRAY\"\t33\n",
      "\"STRANGE\"\t28\n",
      "\"SECOND\"\t21\n",
      "\"NAVIGATOR\"\t18\n",
      "\"VOICE\"\t18\n",
      "\"WALKER\"\t16\n",
      "\"Y-WING\"\t16\n",
      "\"LURE\"\t15\n",
      "\"OPERATOR\"\t9\n"
     ]
    }
   ],
   "source": [
    "!python3 task2.py ../sw-data/SW_EpisodeVI.txt > res6_local.txt && cat res6_local.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68e81f26-30d8-4834-b45a-cf8efb206163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/task2.root.20231206.202857.094559\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/task2.root.20231206.202857.094559/output\n",
      "Streaming final output from /tmp/task2.root.20231206.202857.094559/output...\n",
      "Removing temp directory /tmp/task2.root.20231206.202857.094559...\n",
      "\"BEN\"\t773\n",
      "\"LEIA\"\t596\n",
      "\"ACKBAR\"\t503\n",
      "\"LUKE\"\t502\n",
      "\"MON\"\t460\n",
      "\"YODA\"\t385\n",
      "\"BIGGS\"\t356\n",
      "\"DODONNA\"\t353\n",
      "\"JABBA\"\t339\n",
      "\"TARKIN\"\t302\n",
      "\"THREEPIO\"\t288\n",
      "\"VADER\"\t269\n",
      "\"HAN\"\t256\n",
      "\"MOTTI\"\t228\n",
      "\"EMPEROR\"\t227\n",
      "\"OFFICER\"\t219\n",
      "\"LANDO\"\t206\n",
      "\"VEERS\"\t205\n",
      "\"GREEDO\"\t203\n",
      "\"OWEN\"\t191\n",
      "\"SECOND\"\t187\n",
      "\"TAGGE\"\t183\n",
      "\"GENERAL\"\t183\n",
      "\"FIXER\"\t177\n",
      "\"RED\"\t176\n",
      "\"PIETT\"\t176\n",
      "\"REBEL\"\t167\n",
      "\"COMMANDER\"\t165\n",
      "\"NEEDA\"\t160\n",
      "\"DEATH\"\t158\n",
      "\"VOICE\"\t148\n",
      "\"NINEDENINE\"\t145\n",
      "\"HAN/PILOT\"\t130\n",
      "\"SHUTTLE\"\t124\n",
      "\"RIEEKAN\"\t121\n",
      "\"ZEV\"\t118\n",
      "\"DECK\"\t115\n",
      "\"CREATURE\"\t109\n",
      "\"MASSASSI\"\t107\n",
      "\"HUMAN\"\t107\n",
      "\"TROOPER\"\t101\n",
      "\"CONTROLLER\"\t99\n",
      "\"IMPERIAL\"\t99\n",
      "\"AUNT\"\t98\n",
      "\"WEDGE\"\t98\n",
      "\"CONTROL\"\t97\n",
      "\"ASTRO-OFFICER\"\t96\n",
      "\"GOLD\"\t93\n",
      "\"BASE\"\t91\n",
      "\"WILLARD\"\t90\n",
      "\"OZZEL\"\t88\n",
      "\"DERLIN\"\t88\n",
      "\"GANTRY\"\t88\n",
      "\"INTERCOM\"\t87\n",
      "\"ANAKIN\"\t83\n",
      "\"DACK\"\t82\n",
      "\"LIEUTENANT\"\t78\n",
      "\"CAPTAIN\"\t77\n",
      "\"MAN\"\t76\n",
      "\"JERJERROD\"\t76\n",
      "\"BERU\"\t72\n",
      "\"MEDICAL\"\t72\n",
      "\"FIRST\"\t72\n",
      "\"ASSISTANT\"\t71\n",
      "\"SENIOR\"\t69\n",
      "\"BARTENDER\"\t68\n",
      "\"TRACKING\"\t67\n",
      "\"CHIEF\"\t65\n",
      "\"COMMUNICATIONS\"\t64\n",
      "\"GUARD\"\t62\n",
      "\"BOBA\"\t58\n",
      "\"BOUSHH\"\t54\n",
      "\"PILOT\"\t53\n",
      "\"ANNOUNCER\"\t50\n",
      "\"SCOUT\"\t48\n",
      "\"HEAD\"\t47\n",
      "\"WOMAN\"\t43\n",
      "\"TRENCH\"\t43\n",
      "\"STORMTROOPER\"\t42\n",
      "\"BIB\"\t40\n",
      "\"BUNKER\"\t38\n",
      "\"CAMIE\"\t38\n",
      "\"HOBBIE\"\t38\n",
      "\"OOLA\"\t37\n",
      "\"TECHNICIAN\"\t36\n",
      "\"GREEN\"\t34\n",
      "\"GRAY\"\t33\n",
      "\"STRANGE\"\t28\n",
      "\"JANSON\"\t25\n",
      "\"DEAK\"\t22\n",
      "\"PORKINS\"\t20\n",
      "\"NAVIGATOR\"\t18\n",
      "\"WALKER\"\t16\n",
      "\"Y-WING\"\t16\n",
      "\"LURE\"\t15\n",
      "\"PILOTS\"\t12\n",
      "\"OPERATOR\"\t9\n",
      "\"WINGMAN\"\t9\n"
     ]
    }
   ],
   "source": [
    "!python3 task2.py ../sw-data/SW_full.txt > res_full_local.txt && cat res_full_local.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e26a0560-c0a9-4809-bb50-5d48479f3f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task2.root.20231206.202448.614100\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.202448.614100/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.202448.614100/files/\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar6236805987017176038/] [] /tmp/streamjob8464432376335410464.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701814266085_0023\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701814266085_0023\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701814266085_0023\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701814266085_0023/\n",
      "  Running job: job_1701814266085_0023\n",
      "  Job job_1701814266085_0023 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701814266085_0023 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/task2.root.20231206.202448.614100/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=82374\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=976\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=12881\n",
      "\t\tFILE: Number of bytes written=873834\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=82552\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=976\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3104768\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1286144\n",
      "\t\tTotal time spent by all map tasks (ms)=3032\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6064\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1256\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2512\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3032\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1256\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=950\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=102\n",
      "\t\tInput split bytes=178\n",
      "\t\tMap input records=1011\n",
      "\t\tMap output bytes=10855\n",
      "\t\tMap output materialized bytes=12887\n",
      "\t\tMap output records=1010\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=369270784\n",
      "\t\tPeak Map Virtual memory (bytes)=2483494912\n",
      "\t\tPeak Reduce Physical memory (bytes)=256061440\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2496327680\n",
      "\t\tPhysical memory (bytes) snapshot=994095104\n",
      "\t\tReduce input groups=48\n",
      "\t\tReduce input records=1010\n",
      "\t\tReduce output records=48\n",
      "\t\tReduce shuffle bytes=12887\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=2020\n",
      "\t\tTotal committed heap usage (bytes)=1170210816\n",
      "\t\tVirtual memory (bytes) snapshot=7463018496\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar3496962558816085469/] [] /tmp/streamjob8277294018186629727.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701814266085_0024\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701814266085_0024\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701814266085_0024\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701814266085_0024/\n",
      "  Running job: job_1701814266085_0024\n",
      "  Job job_1701814266085_0024 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701814266085_0024 completed successfully\n",
      "  Output directory: hdfs:///task2_ep4\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1464\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=592\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1078\n",
      "\t\tFILE: Number of bytes written=849952\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1762\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=592\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2822144\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1286144\n",
      "\t\tTotal time spent by all map tasks (ms)=2756\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5512\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1256\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2512\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2756\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1256\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=830\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=104\n",
      "\t\tInput split bytes=298\n",
      "\t\tMap input records=48\n",
      "\t\tMap output bytes=976\n",
      "\t\tMap output materialized bytes=1084\n",
      "\t\tMap output records=48\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=368975872\n",
      "\t\tPeak Map Virtual memory (bytes)=2486415360\n",
      "\t\tPeak Reduce Physical memory (bytes)=256360448\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2492985344\n",
      "\t\tPhysical memory (bytes) snapshot=893636608\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=48\n",
      "\t\tReduce output records=48\n",
      "\t\tReduce shuffle bytes=1084\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=96\n",
      "\t\tTotal committed heap usage (bytes)=1055916032\n",
      "\t\tVirtual memory (bytes) snapshot=7462383616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///task2_ep4\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/task2.root.20231206.202448.614100...\n",
      "Removing temp directory /tmp/task2.root.20231206.202448.614100...\n"
     ]
    }
   ],
   "source": [
    "!python3 task2.py -r hadoop hdfs://namenode:8020/SW_EpisodeIV.txt --output /task2_ep4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "512bfec4-d3f3-4640-bc72-5eaea7747324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task2.root.20231206.202530.160441\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.202530.160441/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.202530.160441/files/\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar4533139531119239227/] [] /tmp/streamjob6404125329565816690.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701814266085_0025\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701814266085_0025\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701814266085_0025\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701814266085_0025/\n",
      "  Running job: job_1701814266085_0025\n",
      "  Job job_1701814266085_0025 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701814266085_0025 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/task2.root.20231206.202530.160441/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=59583\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=917\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=10721\n",
      "\t\tFILE: Number of bytes written=869511\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=59759\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=917\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2878464\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1259520\n",
      "\t\tTotal time spent by all map tasks (ms)=2811\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5622\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1230\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2460\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2811\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1230\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=880\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=92\n",
      "\t\tInput split bytes=176\n",
      "\t\tMap input records=840\n",
      "\t\tMap output bytes=9037\n",
      "\t\tMap output materialized bytes=10727\n",
      "\t\tMap output records=839\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=368304128\n",
      "\t\tPeak Map Virtual memory (bytes)=2482790400\n",
      "\t\tPeak Reduce Physical memory (bytes)=254480384\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2491867136\n",
      "\t\tPhysical memory (bytes) snapshot=990547968\n",
      "\t\tReduce input groups=45\n",
      "\t\tReduce input records=839\n",
      "\t\tReduce output records=45\n",
      "\t\tReduce shuffle bytes=10727\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1678\n",
      "\t\tTotal committed heap usage (bytes)=1173880832\n",
      "\t\tVirtual memory (bytes) snapshot=7457292288\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar5728890869276519821/] [] /tmp/streamjob5031952721274798801.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701814266085_0026\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701814266085_0026\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701814266085_0026\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701814266085_0026/\n",
      "  Running job: job_1701814266085_0026\n",
      "  Job job_1701814266085_0026 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701814266085_0026 completed successfully\n",
      "  Output directory: hdfs:///task2_ep5\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1376\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=557\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1013\n",
      "\t\tFILE: Number of bytes written=849822\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1674\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=557\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2725888\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1211392\n",
      "\t\tTotal time spent by all map tasks (ms)=2662\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5324\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1183\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2366\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2662\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1183\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=750\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=89\n",
      "\t\tInput split bytes=298\n",
      "\t\tMap input records=45\n",
      "\t\tMap output bytes=917\n",
      "\t\tMap output materialized bytes=1019\n",
      "\t\tMap output records=45\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=368971776\n",
      "\t\tPeak Map Virtual memory (bytes)=2482667520\n",
      "\t\tPeak Reduce Physical memory (bytes)=256274432\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2491760640\n",
      "\t\tPhysical memory (bytes) snapshot=993251328\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=45\n",
      "\t\tReduce output records=45\n",
      "\t\tReduce shuffle bytes=1019\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=90\n",
      "\t\tTotal committed heap usage (bytes)=1170210816\n",
      "\t\tVirtual memory (bytes) snapshot=7456993280\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///task2_ep5\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/task2.root.20231206.202530.160441...\n",
      "Removing temp directory /tmp/task2.root.20231206.202530.160441...\n"
     ]
    }
   ],
   "source": [
    "!python3 task2.py -r hadoop hdfs://namenode:8020/SW_EpisodeV.txt --output /task2_ep5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "716a770c-f0e0-4776-a274-51762f3a6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task2.root.20231206.202610.437581\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.202610.437581/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.202610.437581/files/\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar423564120774821486/] [] /tmp/streamjob4786447461401565650.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701814266085_0027\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701814266085_0027\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701814266085_0027\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701814266085_0027/\n",
      "  Running job: job_1701814266085_0027\n",
      "  Job job_1701814266085_0027 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701814266085_0027 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/task2.root.20231206.202610.437581/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=52272\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=914\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8874\n",
      "\t\tFILE: Number of bytes written=865820\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=52450\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=914\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3118080\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1262592\n",
      "\t\tTotal time spent by all map tasks (ms)=3045\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6090\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1233\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2466\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3045\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1233\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=860\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=91\n",
      "\t\tInput split bytes=178\n",
      "\t\tMap input records=675\n",
      "\t\tMap output bytes=7520\n",
      "\t\tMap output materialized bytes=8880\n",
      "\t\tMap output records=674\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=369184768\n",
      "\t\tPeak Map Virtual memory (bytes)=2486878208\n",
      "\t\tPeak Reduce Physical memory (bytes)=256598016\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2492092416\n",
      "\t\tPhysical memory (bytes) snapshot=994140160\n",
      "\t\tReduce input groups=45\n",
      "\t\tReduce input records=674\n",
      "\t\tReduce output records=45\n",
      "\t\tReduce shuffle bytes=8880\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1348\n",
      "\t\tTotal committed heap usage (bytes)=1166540800\n",
      "\t\tVirtual memory (bytes) snapshot=7461146624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar2248388893764628638/] [] /tmp/streamjob253818110903058591.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701814266085_0028\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701814266085_0028\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701814266085_0028\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701814266085_0028/\n",
      "  Running job: job_1701814266085_0028\n",
      "  Job job_1701814266085_0028 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701814266085_0028 completed successfully\n",
      "  Output directory: hdfs:///task2_ep6\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1371\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=554\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1010\n",
      "\t\tFILE: Number of bytes written=849813\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1669\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=554\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3679232\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1536000\n",
      "\t\tTotal time spent by all map tasks (ms)=3593\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7186\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1500\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3000\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3593\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1500\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1090\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=159\n",
      "\t\tInput split bytes=298\n",
      "\t\tMap input records=45\n",
      "\t\tMap output bytes=914\n",
      "\t\tMap output materialized bytes=1016\n",
      "\t\tMap output records=45\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=369721344\n",
      "\t\tPeak Map Virtual memory (bytes)=2483728384\n",
      "\t\tPeak Reduce Physical memory (bytes)=256151552\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2491420672\n",
      "\t\tPhysical memory (bytes) snapshot=994865152\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=45\n",
      "\t\tReduce output records=45\n",
      "\t\tReduce shuffle bytes=1016\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=90\n",
      "\t\tTotal committed heap usage (bytes)=1193279488\n",
      "\t\tVirtual memory (bytes) snapshot=7457865728\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///task2_ep6\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/task2.root.20231206.202610.437581...\n",
      "Removing temp directory /tmp/task2.root.20231206.202610.437581...\n"
     ]
    }
   ],
   "source": [
    "!python3 task2.py -r hadoop hdfs://namenode:8020/SW_EpisodeVI.txt --output /task2_ep6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d790d75c-9898-4b53-9b80-d4f6c6f790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/task2.root.20231206.202652.631164\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.202652.631164/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/task2.root.20231206.202652.631164/files/\n",
      "Running step 1 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar4042618446059329925/] [] /tmp/streamjob1666460385831025531.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701814266085_0029\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701814266085_0029\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701814266085_0029\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701814266085_0029/\n",
      "  Running job: job_1701814266085_0029\n",
      "  Job job_1701814266085_0029 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701814266085_0029 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/task2.root.20231206.202652.631164/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=186037\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2008\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=32464\n",
      "\t\tFILE: Number of bytes written=912985\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=186205\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=2008\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=3202048\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1347584\n",
      "\t\tTotal time spent by all map tasks (ms)=3127\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6254\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1316\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2632\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=3127\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1316\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=980\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=100\n",
      "\t\tInput split bytes=168\n",
      "\t\tMap input records=2526\n",
      "\t\tMap output bytes=27412\n",
      "\t\tMap output materialized bytes=32470\n",
      "\t\tMap output records=2523\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=370040832\n",
      "\t\tPeak Map Virtual memory (bytes)=2484502528\n",
      "\t\tPeak Reduce Physical memory (bytes)=256110592\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2491985920\n",
      "\t\tPhysical memory (bytes) snapshot=996106240\n",
      "\t\tReduce input groups=98\n",
      "\t\tReduce input records=2523\n",
      "\t\tReduce output records=98\n",
      "\t\tReduce shuffle bytes=32470\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=5046\n",
      "\t\tTotal committed heap usage (bytes)=1171783680\n",
      "\t\tVirtual memory (bytes) snapshot=7459344384\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [/tmp/hadoop-unjar7033206921660228681/] [] /tmp/streamjob5568132176166947743.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.21.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701814266085_0030\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701814266085_0030\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701814266085_0030\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701814266085_0030/\n",
      "  Running job: job_1701814266085_0030\n",
      "  Job job_1701814266085_0030 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701814266085_0030 completed successfully\n",
      "  Output directory: hdfs:///task2_full\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3012\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1224\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2210\n",
      "\t\tFILE: Number of bytes written=852219\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3310\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=1224\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2992128\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=1461248\n",
      "\t\tTotal time spent by all map tasks (ms)=2922\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5844\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1427\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2854\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=2922\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1427\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=860\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=111\n",
      "\t\tInput split bytes=298\n",
      "\t\tMap input records=98\n",
      "\t\tMap output bytes=2008\n",
      "\t\tMap output materialized bytes=2216\n",
      "\t\tMap output records=98\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=369287168\n",
      "\t\tPeak Map Virtual memory (bytes)=2482696192\n",
      "\t\tPeak Reduce Physical memory (bytes)=255647744\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2495647744\n",
      "\t\tPhysical memory (bytes) snapshot=992923648\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=98\n",
      "\t\tReduce output records=98\n",
      "\t\tReduce shuffle bytes=2216\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=196\n",
      "\t\tTotal committed heap usage (bytes)=1175453696\n",
      "\t\tVirtual memory (bytes) snapshot=7460724736\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///task2_full\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/task2.root.20231206.202652.631164...\n",
      "Removing temp directory /tmp/task2.root.20231206.202652.631164...\n"
     ]
    }
   ],
   "source": [
    "!python3 task2.py -r hadoop hdfs://namenode:8020/SW_full.txt --output /task2_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0c19980-234a-47e8-8ffb-3a7f55bf9be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-06 23:27:53 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"LEIA\"\t596\n",
      "\"BIGGS\"\t356\n",
      "\"DODONNA\"\t353\n",
      "\"JABBA\"\t339\n",
      "\"LUKE\"\t318\n",
      "\"TARKIN\"\t302\n",
      "\"THREEPIO\"\t288\n",
      "\"BEN\"\t262\n",
      "\"VADER\"\t257\n",
      "\"HAN\"\t256\n",
      "\"MOTTI\"\t228\n",
      "\"OFFICER\"\t219\n",
      "\"GREEDO\"\t203\n",
      "\"OWEN\"\t191\n",
      "\"SECOND\"\t187\n",
      "\"TAGGE\"\t183\n",
      "\"FIXER\"\t177\n",
      "\"RED\"\t176\n",
      "\"VOICE\"\t148\n",
      "\"DEATH\"\t143\n",
      "\"COMMANDER\"\t112\n",
      "\"MASSASSI\"\t107\n",
      "\"HUMAN\"\t107\n",
      "\"REBEL\"\t106\n",
      "\"TROOPER\"\t101\n",
      "\"IMPERIAL\"\t99\n",
      "\"WEDGE\"\t98\n",
      "\"AUNT\"\t98\n",
      "\"CONTROL\"\t97\n",
      "\"ASTRO-OFFICER\"\t96\n",
      "\"GOLD\"\t93\n",
      "\"BASE\"\t91\n",
      "\"WILLARD\"\t90\n",
      "\"GANTRY\"\t88\n",
      "\"INTERCOM\"\t87\n",
      "\"CAPTAIN\"\t77\n",
      "\"MAN\"\t76\n",
      "\"FIRST\"\t72\n",
      "\"BERU\"\t72\n",
      "\"BARTENDER\"\t68\n",
      "\"CHIEF\"\t65\n",
      "\"CAMIE\"\t38\n",
      "\"TECHNICIAN\"\t36\n",
      "\"WOMAN\"\t32\n",
      "\"CREATURE\"\t28\n",
      "\"DEAK\"\t22\n",
      "\"PORKINS\"\t20\n",
      "\"WINGMAN\"\t9\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -get /task2_ep4/part-00000 ./res_ep4_hadoop.txt && cat ./res_ep4_hadoop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c656da7-d898-4bb2-96db-a332aab74438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-06 23:27:54 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"YODA\"\t385\n",
      "\"VADER\"\t269\n",
      "\"LEIA\"\t264\n",
      "\"THREEPIO\"\t238\n",
      "\"LANDO\"\t206\n",
      "\"VEERS\"\t205\n",
      "\"LUKE\"\t198\n",
      "\"PIETT\"\t176\n",
      "\"REBEL\"\t167\n",
      "\"NEEDA\"\t160\n",
      "\"HAN\"\t152\n",
      "\"RIEEKAN\"\t121\n",
      "\"ZEV\"\t118\n",
      "\"BEN\"\t117\n",
      "\"DECK\"\t115\n",
      "\"CREATURE\"\t109\n",
      "\"OZZEL\"\t88\n",
      "\"DERLIN\"\t88\n",
      "\"CONTROLLER\"\t85\n",
      "\"DACK\"\t82\n",
      "\"LIEUTENANT\"\t78\n",
      "\"SECOND\"\t77\n",
      "\"EMPEROR\"\t75\n",
      "\"MEDICAL\"\t72\n",
      "\"ASSISTANT\"\t71\n",
      "\"SENIOR\"\t69\n",
      "\"TRACKING\"\t67\n",
      "\"INTERCOM\"\t64\n",
      "\"COMMUNICATIONS\"\t64\n",
      "\"BOBA\"\t58\n",
      "\"IMPERIAL\"\t53\n",
      "\"ANNOUNCER\"\t50\n",
      "\"HEAD\"\t47\n",
      "\"WOMAN\"\t43\n",
      "\"WEDGE\"\t43\n",
      "\"TRENCH\"\t43\n",
      "\"HOBBIE\"\t38\n",
      "\"CAPTAIN\"\t33\n",
      "\"PILOT\"\t29\n",
      "\"JANSON\"\t25\n",
      "\"STRANGE\"\t23\n",
      "\"FIRST\"\t22\n",
      "\"OFFICER\"\t14\n",
      "\"PILOTS\"\t12\n",
      "\"MAN\"\t12\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -get /task2_ep5/part-00000 ./res_ep5_hadoop.txt && cat ./res_ep5_hadoop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c410b5c-54a1-4c81-bfbd-6f3b043d9c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-06 23:27:56 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"BEN\"\t773\n",
      "\"ACKBAR\"\t503\n",
      "\"LUKE\"\t502\n",
      "\"MON\"\t460\n",
      "\"YODA\"\t293\n",
      "\"EMPEROR\"\t227\n",
      "\"VADER\"\t207\n",
      "\"THREEPIO\"\t183\n",
      "\"GENERAL\"\t183\n",
      "\"COMMANDER\"\t165\n",
      "\"DEATH\"\t158\n",
      "\"HAN\"\t154\n",
      "\"LANDO\"\t149\n",
      "\"NINEDENINE\"\t145\n",
      "\"HAN/PILOT\"\t130\n",
      "\"PIETT\"\t126\n",
      "\"SHUTTLE\"\t124\n",
      "\"LEIA\"\t110\n",
      "\"CONTROLLER\"\t99\n",
      "\"JABBA\"\t94\n",
      "\"ANAKIN\"\t83\n",
      "\"WEDGE\"\t79\n",
      "\"JERJERROD\"\t76\n",
      "\"CONTROL\"\t63\n",
      "\"GUARD\"\t62\n",
      "\"OFFICER\"\t59\n",
      "\"BOUSHH\"\t54\n",
      "\"PILOT\"\t53\n",
      "\"SCOUT\"\t48\n",
      "\"RED\"\t48\n",
      "\"STORMTROOPER\"\t42\n",
      "\"BIB\"\t40\n",
      "\"BUNKER\"\t38\n",
      "\"OOLA\"\t37\n",
      "\"GREEN\"\t34\n",
      "\"REBEL\"\t33\n",
      "\"GRAY\"\t33\n",
      "\"STRANGE\"\t28\n",
      "\"SECOND\"\t21\n",
      "\"VOICE\"\t18\n",
      "\"NAVIGATOR\"\t18\n",
      "\"Y-WING\"\t16\n",
      "\"WALKER\"\t16\n",
      "\"LURE\"\t15\n",
      "\"OPERATOR\"\t9\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -get /task2_ep6/part-00000 ./res_ep6_hadoop.txt && cat ./res_ep6_hadoop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd876d96-94da-47d4-8d64-6a8da9825ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-06 23:27:56 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"BEN\"\t773\n",
      "\"LEIA\"\t596\n",
      "\"ACKBAR\"\t503\n",
      "\"LUKE\"\t502\n",
      "\"MON\"\t460\n",
      "\"YODA\"\t385\n",
      "\"BIGGS\"\t356\n",
      "\"DODONNA\"\t353\n",
      "\"JABBA\"\t339\n",
      "\"TARKIN\"\t302\n",
      "\"THREEPIO\"\t288\n",
      "\"VADER\"\t269\n",
      "\"HAN\"\t256\n",
      "\"MOTTI\"\t228\n",
      "\"EMPEROR\"\t227\n",
      "\"OFFICER\"\t219\n",
      "\"LANDO\"\t206\n",
      "\"VEERS\"\t205\n",
      "\"GREEDO\"\t203\n",
      "\"OWEN\"\t191\n",
      "\"SECOND\"\t187\n",
      "\"TAGGE\"\t183\n",
      "\"GENERAL\"\t183\n",
      "\"FIXER\"\t177\n",
      "\"RED\"\t176\n",
      "\"PIETT\"\t176\n",
      "\"REBEL\"\t167\n",
      "\"COMMANDER\"\t165\n",
      "\"NEEDA\"\t160\n",
      "\"DEATH\"\t158\n",
      "\"VOICE\"\t148\n",
      "\"NINEDENINE\"\t145\n",
      "\"HAN/PILOT\"\t130\n",
      "\"SHUTTLE\"\t124\n",
      "\"RIEEKAN\"\t121\n",
      "\"ZEV\"\t118\n",
      "\"DECK\"\t115\n",
      "\"CREATURE\"\t109\n",
      "\"MASSASSI\"\t107\n",
      "\"HUMAN\"\t107\n",
      "\"TROOPER\"\t101\n",
      "\"IMPERIAL\"\t99\n",
      "\"CONTROLLER\"\t99\n",
      "\"WEDGE\"\t98\n",
      "\"AUNT\"\t98\n",
      "\"CONTROL\"\t97\n",
      "\"ASTRO-OFFICER\"\t96\n",
      "\"GOLD\"\t93\n",
      "\"BASE\"\t91\n",
      "\"WILLARD\"\t90\n",
      "\"OZZEL\"\t88\n",
      "\"GANTRY\"\t88\n",
      "\"DERLIN\"\t88\n",
      "\"INTERCOM\"\t87\n",
      "\"ANAKIN\"\t83\n",
      "\"DACK\"\t82\n",
      "\"LIEUTENANT\"\t78\n",
      "\"CAPTAIN\"\t77\n",
      "\"MAN\"\t76\n",
      "\"JERJERROD\"\t76\n",
      "\"MEDICAL\"\t72\n",
      "\"FIRST\"\t72\n",
      "\"BERU\"\t72\n",
      "\"ASSISTANT\"\t71\n",
      "\"SENIOR\"\t69\n",
      "\"BARTENDER\"\t68\n",
      "\"TRACKING\"\t67\n",
      "\"CHIEF\"\t65\n",
      "\"COMMUNICATIONS\"\t64\n",
      "\"GUARD\"\t62\n",
      "\"BOBA\"\t58\n",
      "\"BOUSHH\"\t54\n",
      "\"PILOT\"\t53\n",
      "\"ANNOUNCER\"\t50\n",
      "\"SCOUT\"\t48\n",
      "\"HEAD\"\t47\n",
      "\"WOMAN\"\t43\n",
      "\"TRENCH\"\t43\n",
      "\"STORMTROOPER\"\t42\n",
      "\"BIB\"\t40\n",
      "\"HOBBIE\"\t38\n",
      "\"CAMIE\"\t38\n",
      "\"BUNKER\"\t38\n",
      "\"OOLA\"\t37\n",
      "\"TECHNICIAN\"\t36\n",
      "\"GREEN\"\t34\n",
      "\"GRAY\"\t33\n",
      "\"STRANGE\"\t28\n",
      "\"JANSON\"\t25\n",
      "\"DEAK\"\t22\n",
      "\"PORKINS\"\t20\n",
      "\"NAVIGATOR\"\t18\n",
      "\"Y-WING\"\t16\n",
      "\"WALKER\"\t16\n",
      "\"LURE\"\t15\n",
      "\"PILOTS\"\t12\n",
      "\"WINGMAN\"\t9\n",
      "\"OPERATOR\"\t9\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -get /task2_full/part-00000 ./res_full_hadoop.txt && cat ./res_full_hadoop.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
